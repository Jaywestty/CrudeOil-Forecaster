# ğŸ›¢ï¸ Crude Oil Price Scenario Forecaster

> **Macroeconomic simulation engine for Brent crude oil pricing â€” powered by SARIMAX econometrics and LLaMA 3.3 via Groq.**

[![Live Demo](https://img.shields.io/badge/Live%20Demo-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit)](https://crude-forecaster.streamlit.app/)
[![API Docs](https://img.shields.io/badge/API%20Docs-FastAPI-009688?style=for-the-badge&logo=fastapi)](https://crudeoil-forecaster.onrender.com/docs)

---

## ğŸŒ Live Deployments

| Service | URL | Description |
|---|---|---|
| ğŸ–¥ï¸ Frontend (Streamlit) | [crude-forecaster.streamlit.app](https://crude-forecaster.streamlit.app/) | Interactive scenario simulation UI |
| âš™ï¸ Backend (FastAPI) | [crudeoil-forecaster.onrender.com](https://crudeoil-forecaster.onrender.com/) | REST API + auto-generated docs at `/docs` |

> âš ï¸ **Note on cold starts:** The API is hosted on Render's free tier, which **spins down after 15 minutes of inactivity** to save resources. The first request after a period of sleep may take **30â€“60 seconds** to wake up â€” this is expected behaviour, not a bug. Just wait a moment and it will respond normally.

---

## ğŸ“Œ What This Project Does

This system answers questions like:

- *"What happens to oil prices if OPEC cuts production by 10%?"*
- *"Simulate a global recession scenario"*
- *"What if the US Federal Reserve raises interest rates aggressively?"*
- *"What if there's a geopolitical conflict in the Middle East?"*

You type a natural language question. The system:
1. Uses **LLaMA 3.3 70B** (via Groq) to parse your intent into structured economic shock parameters
2. Injects those shocks into a **SARIMAX econometric model** trained on 20 years of weekly oil market data
3. Generates a **baseline forecast** (what happens with no shock) vs a **counterfactual forecast** (what happens with your scenario)
4. Uses the LLM again to explain the results in plain economic language

---

## ğŸ—ï¸ System Architecture

```
User types scenario query (natural language)
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Streamlit UI   â”‚  â† crude-forecaster.streamlit.app
   â”‚   (Frontend)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚  HTTP POST /simulate
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   FastAPI API   â”‚  â† crudeoil-forecaster.onrender.com
   â”‚   (Backend)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SARIMAX â”‚  â”‚  Groq    â”‚
â”‚  Model  â”‚  â”‚ LLaMA 3.3â”‚
â”‚ (.pkl)  â”‚  â”‚  (LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â–¼
   Forecast + Explanation
   returned to Streamlit UI
```

---

## ğŸ§  Why SARIMAX? (Modeling Decision)

Three models were considered. Here's why SARIMAX won:

| Model | Strength | Why Not Used |
|---|---|---|
| **SARIMA** | Clean time series structure | No external variable support â€” can't inject macro shocks |
| **XGBoost** | Powerful, wins Kaggle competitions | Treats observations as independent, no temporal structure, no counterfactual mechanism |
| **Prophet** | Great for calendar seasonality | Built for daily business metrics, poor exogenous variable support, no economic interpretability |
| âœ… **SARIMAX** | Time structure + external variables + interpretable coefficients | **Chosen** |

The **X in SARIMAX** (eXogenous variables) is the entire reason it was chosen. It allows injecting macro shocks â€” dollar returns, inventory changes, VIX spikes â€” directly into the forecast. Without the X, scenario simulation is impossible.

**Key model coefficient (defend this in interview):**
```
dollar_return = -107.80 (p < 0.001)
â†’ "A 1% appreciation in the US dollar
   corresponds to a $1.08/barrel drop in Brent crude"
â†’ This is economically interpretable and statistically significant
```

---

## ğŸ“Š Model Specification

```
Model:          SARIMAX(1, 1, 1)(1, 0, 1, 52)
                 â”‚  â”‚  â”‚   â”‚  â”‚  â”‚   52 = weekly seasonality
                 â”‚  â”‚  â”‚   â””â”€â”€â”´â”€â”€â”˜ seasonal AR, I, MA terms
                 â””â”€â”€â”´â”€â”€â”˜ non-seasonal AR, differencing, MA terms

Training data:  2006 â€“ 2022  (80% of 1,043 weekly observations)
Test period:    2022 â€“ 2024  (20% holdout)
Validation:     Rolling window cross-validation (10 folds)
Test MAPE:      15.22%
AIC:            Minimized via order selection

Exogenous variables (the 5 macro drivers):
  dollar_return   â†’ DXY US Dollar Index weekly return
  indpro_return   â†’ US Industrial Production weekly change
  inventory_pct   â†’ US crude oil inventory % change (EIA)
  fed_funds_diff  â†’ Federal Funds Rate weekly difference
  vix_diff        â†’ CBOE VIX (fear index) weekly difference
```

---

## ğŸ”„ Simulation Pipeline (Step by Step)

When you click **RUN SIMULATION**, here is exactly what happens under the hood:

```
Step 1 â€” LLM Query Parsing
  Your natural language query â†’
  Groq API (LLaMA 3.3 70B) â†’
  Structured JSON: { scenario_key, magnitude_modifier, confidence, reasoning }

Step 2 â€” Shock Construction
  Predefined scenario shocks Ã— magnitude_modifier =
  calibrated shock vector for each of the 5 macro variables

Step 3 â€” Baseline Forecast
  SARIMAX model â†’ forecast with ZERO shocks applied â†’
  "what happens if nothing changes"

Step 4 â€” Counterfactual Forecast
  SARIMAX model â†’ same forecast WITH shocks injected â†’
  "what happens under your scenario"

Step 5 â€” LLM Explanation
  Both forecasts + scenario metadata â†’
  Groq API â†’
  Plain English economic explanation + uncertainty note

Step 6 â€” Response Assembly
  Weekly forecast table, impact at Week 1 and Week 12,
  shock breakdown, explanation â†’ returned to Streamlit UI
```

---

## ğŸ—‚ï¸ Project Structure

```
crude-oil-forecaster/
â”‚
â”œâ”€â”€ api.py                  # FastAPI backend â€” all REST endpoints
â”œâ”€â”€ streamlit_app.py        # Streamlit frontend â€” the UI
â”œâ”€â”€ scenario_engine.py      # Core simulation logic â€” runs SARIMAX
â”œâ”€â”€ llm_explainer.py        # Groq/LLaMA interface â€” parsing + explanation
â”œâ”€â”€ utils.py                # Model loading, data helpers
â”œâ”€â”€ train.py                # Model training script (runs on Render build)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ sarimax_model.pkl   # Generated at build time â€” not in GitHub
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ oil_macro_weekly.csv        # Raw weekly data (2004â€“2024)
â”‚   â””â”€â”€ oil_macro_transformed.csv  # Stationary transformed features
â”‚
â”œâ”€â”€ requirements.txt        # Python dependencies (pinned for stability)
â”œâ”€â”€ .python-version         # Pins Python 3.11.8 â€” fixes Render build issues
â””â”€â”€ .env                    # Local secrets (never committed)
```

---

## ğŸš€ API Endpoints

Base URL: `https://crudeoil-forecaster.onrender.com`

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/` | Health check |
| `GET` | `/docs` | Interactive API documentation (Swagger UI) |
| `GET` | `/scenarios` | List all predefined scenarios |
| `GET` | `/current-price` | Latest Brent crude price from dataset |
| `POST` | `/simulate` | **Main endpoint** â€” run full NL simulation |
| `POST` | `/simulate-direct` | Run simulation by scenario key (bypasses LLM) |

### Example Request

```bash
curl -X POST "https://crudeoil-forecaster.onrender.com/simulate" \
  -H "Content-Type: application/json" \
  -d '{"query": "What if OPEC cuts production by 10%?", "forecast_weeks": 12}'
```

### Example Response (abbreviated)

```json
{
  "scenario_name":     "OPEC Production Cut",
  "current_price":     74.23,
  "parsed_confidence": "high",
  "impact_week1":  { "difference": 3.42, "pct_change": 4.61 },
  "impact_week12": { "difference": 7.18, "pct_change": 9.67 },
  "explanation":   "An OPEC production cut of this magnitude...",
  "weekly_forecasts": [...]
}
```

---

## ğŸ› ï¸ Local Development Setup

### Prerequisites

- Python 3.11
- A [Groq API key](https://console.groq.com) (free)
- A [FRED API key](https://fredaccount.stlouisfed.org) (free)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/Jaywestty/CrudeOil-Forecaster.git
cd CrudeOil-Forecaster

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up environment variables
# Create a .env file in the project root:
GROQ_API_KEY=your_groq_key_here
FRED_API_KEY=your_fred_key_here

# 5. Train the model (only needed once)
python train.py

# 6. Start the FastAPI backend
uvicorn api:app --reload
# API now running at http://localhost:8000
# Docs at http://localhost:8000/docs

# 7. In a new terminal, start the Streamlit frontend
streamlit run streamlit_app.py
# UI now running at http://localhost:8501
```

---

## ğŸ“¦ Data Sources

| Variable | Source | Frequency | Description |
|---|---|---|---|
| Brent Crude Price | EIA / FRED | Weekly | USD per barrel |
| US Dollar Index | FRED (DTWEXBGS) | Weekly | DXY trade-weighted index |
| Industrial Production | FRED (INDPRO) | Monthly â†’ Weekly | US manufacturing output |
| Crude Inventories | EIA | Weekly | US commercial crude stocks |
| Federal Funds Rate | FRED (FEDFUNDS) | Monthly â†’ Weekly | US benchmark interest rate |
| VIX | FRED (VIXCLS) | Daily â†’ Weekly | CBOE volatility / fear index |

---

## ğŸ§ª Model Validation

```
Validation strategy: Rolling window cross-validation
  10 folds across the test period (2022â€“2024)
  Each fold trains on all data up to that point
  Forecasts 12 weeks ahead out-of-sample

Results:
  Test MAPE:  15.22%
  Benchmark:  ARIMA baseline (univariate, no macro variables)
  SARIMAX outperforms ARIMA baseline on directional accuracy
  and responds meaningfully to macro variable shocks

Note on MAPE: 15.22% on weekly oil prices is reasonable.
Oil is one of the most volatile commodities in the world â€”
affected by geopolitical events, OPEC decisions, and
macroeconomic shocks that no statistical model can fully anticipate.
```

---

## âš™ï¸ Deployment Architecture

### Backend â€” Render (FastAPI)

```
Platform:       Render Web Service (Free Tier)
Runtime:        Python 3.11
Build Command:  pip install -r requirements.txt && python train.py
Start Command:  uvicorn api:app --host 0.0.0.0 --port $PORT
Model Storage:  Trained fresh on every deploy â€” no binary in GitHub
Auto-deploy:    Yes â€” triggers on every push to main branch

Build Command explained:
  pip install -r requirements.txt   â†’ install dependencies
  && python train.py                â†’ train SARIMAX, save .pkl to disk
  &&                                â†’ only trains if install succeeded

Free Tier Behaviour:
  âœ… 512MB RAM â€” sufficient for SARIMAX model in memory
  âœ… Shared CPU â€” adequate for inference
  âš ï¸  Spins down after 15 min of inactivity
  âš ï¸  Cold start takes 30â€“60 seconds to wake up
  âš ï¸  Build takes ~5â€“10 min longer due to training step
```

### Frontend â€” Streamlit Cloud

```
Platform:       Streamlit Community Cloud (Free)
Runtime:        Python 3.11
Entry point:    streamlit_app.py
Secrets:        API_URL set via Streamlit Cloud dashboard
Auto-deploy:    Yes â€” triggers on every push to main branch
```

### Model Training on Render

```
Problem:  SARIMAX model = 300MB â€” too large to store in GitHub
Solution: Train the model fresh on every Render deploy

How it works:
  Build Command runs two steps in sequence:
    pip install -r requirements.txt   â† install all dependencies
    && python train.py                â† train SARIMAX and save .pkl

  The && means: "only run train.py if pip install succeeded"

  Render trains the model once per deploy, saves it to disk,
  and the running API loads it from there.
  No large files in GitHub. No LFS needed.

Trade-off:
  âœ… Clean â€” no binary files in version control
  âœ… Model always reflects the latest data and code
  âš ï¸  Adds ~5â€“10 minutes to build time on each deploy
  âš ï¸  Depends on FRED/EIA API availability at build time
```

---

## ğŸ” Environment Variables

| Variable | Where to Set | Description |
|---|---|---|
| `GROQ_API_KEY` | Render dashboard â†’ Environment | LLaMA 3.3 access via Groq |
| `FRED_API_KEY` | Render dashboard â†’ Environment | FRED macroeconomic data |
| `API_URL` | Streamlit Cloud â†’ Secrets | Render backend URL |

**Never commit `.env` to GitHub.** It is in `.gitignore`.

---

## ğŸ’¬ Predefined Scenarios

| Scenario Key | Name | Description |
|---|---|---|
| `opec_cut` | OPEC Production Cut | OPEC reduces output, supply tightens |
| `us_recession` | US Recession | Demand collapses, industrial output drops |
| `dollar_surge` | Dollar Surge | USD strengthens, oil becomes expensive to import |
| `middle_east_conflict` | Middle East Conflict | Supply disruption risk premium spikes |
| `fed_hike` | Fed Rate Hike | Higher rates slow growth, reduce demand |
| `shale_boom` | US Shale Boom | Supply surge from US production |

---

## ğŸ§© Technical Decisions & Trade-offs

**Why not XGBoost or Prophet?**
XGBoost treats observations as independent rows â€” it has no concept of time order or seasonality. Prophet is designed for daily business metrics with calendar patterns (Black Friday, Christmas). Neither provides a native mechanism for injecting economic shocks into future forecasts. SARIMAX was built for exactly this problem.

**Why separate frontend and backend?**
Decoupling the API from the UI means the API can be called by any client â€” another frontend, a mobile app, a Jupyter notebook, or a direct curl command. This is standard production architecture. The Streamlit UI is one consumer of the API, not tightly coupled to it.

**Why Groq for the LLM?**
Groq's inference hardware (LPU) is significantly faster than standard GPU inference â€” LLaMA 3.3 70B responses come back in under 2 seconds. For a real-time UI, this latency matters. OpenAI GPT-4 at similar capability costs more and responds slower on free tiers.

**Why train the model on Render instead of committing it to GitHub?**
The trained SARIMAX model file is 300MB â€” well above GitHub's 100MB hard limit. Rather than using Git LFS or S3, the model is retrained fresh on every Render deploy via `pip install -r requirements.txt && python train.py` in the build command. This keeps the repository clean (no binary files in version control), ensures the model always reflects the latest code, and avoids any external storage dependency. The trade-off is a longer build time (~5â€“10 minutes), which is acceptable for a deployment that rarely changes.

---

## ğŸ“ License

MIT License â€” free to use, modify, and distribute.

---

*Built as a technical assessment â€” demonstrating end-to-end ML system design from data ingestion through econometric modeling, LLM integration, API development, and cloud deployment.*
